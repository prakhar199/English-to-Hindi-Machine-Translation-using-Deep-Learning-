{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5GMMYOF4zwF",
        "outputId": "d12ce46a-ca90-4b7e-b613-8674eb9b55f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.63.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "octej0Xd495s"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVMdiB1g5dpd",
        "outputId": "f257919a-560c-4e04-e2dd-7750d2ca7935"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading hindienglish-corpora.zip to /content\n",
            " 65% 9.00M/13.9M [00:00<00:00, 89.9MB/s]\n",
            "100% 13.9M/13.9M [00:00<00:00, 89.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d aiswaryaramachandran/hindienglish-corpora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLi2mxQwA0e4",
        "outputId": "aa84e51f-803c-4b3e-a08f-9dfd616a332d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/hindienglish-corpora.zip\n",
            "  inflating: Hindi_English_Truncated_Corpus.csv  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/hindienglish-corpora.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "jL7L1hHPA80I",
        "outputId": "2a31dc37-f34b-4d76-b9aa-af6358a9affa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-83e7b218-bdaf-4b0d-8029-cfee91c4c13b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>english_sentence</th>\n",
              "      <th>hindi_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ted</td>\n",
              "      <td>politicians do not have permission to do what ...</td>\n",
              "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ted</td>\n",
              "      <td>I'd like to tell you about one such child,</td>\n",
              "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>indic2012</td>\n",
              "      <td>This percentage is even greater than the perce...</td>\n",
              "      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ted</td>\n",
              "      <td>what we really mean is that they're bad at not...</td>\n",
              "      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>indic2012</td>\n",
              "      <td>.The ending portion of these Vedas is called U...</td>\n",
              "      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127602</th>\n",
              "      <td>indic2012</td>\n",
              "      <td>Examples of art deco construction can be found...</td>\n",
              "      <td>आर्ट डेको शैली के निर्माण मैरीन ड्राइव और ओवल ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127603</th>\n",
              "      <td>ted</td>\n",
              "      <td>and put it in our cheeks.</td>\n",
              "      <td>और अपने गालों में डाल लेते हैं।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127604</th>\n",
              "      <td>tides</td>\n",
              "      <td>As for the other derivatives of sulphur , the ...</td>\n",
              "      <td>जहां तक गंधक के अन्य उत्पादों का प्रश्न है , द...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127605</th>\n",
              "      <td>tides</td>\n",
              "      <td>its complicated functioning is defined thus in...</td>\n",
              "      <td>Zरचना-प्रकिया को उसने एक पहेली में यों बांधा है .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127606</th>\n",
              "      <td>ted</td>\n",
              "      <td>They've just won four government contracts to ...</td>\n",
              "      <td>हाल ही में उन्हें सरकारी ठेका मिला है करीब सौ ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>127607 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-83e7b218-bdaf-4b0d-8029-cfee91c4c13b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-83e7b218-bdaf-4b0d-8029-cfee91c4c13b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-83e7b218-bdaf-4b0d-8029-cfee91c4c13b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           source                                   english_sentence  \\\n",
              "0             ted  politicians do not have permission to do what ...   \n",
              "1             ted         I'd like to tell you about one such child,   \n",
              "2       indic2012  This percentage is even greater than the perce...   \n",
              "3             ted  what we really mean is that they're bad at not...   \n",
              "4       indic2012  .The ending portion of these Vedas is called U...   \n",
              "...           ...                                                ...   \n",
              "127602  indic2012  Examples of art deco construction can be found...   \n",
              "127603        ted                          and put it in our cheeks.   \n",
              "127604      tides  As for the other derivatives of sulphur , the ...   \n",
              "127605      tides  its complicated functioning is defined thus in...   \n",
              "127606        ted  They've just won four government contracts to ...   \n",
              "\n",
              "                                           hindi_sentence  \n",
              "0       राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...  \n",
              "1       मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...  \n",
              "2        यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।  \n",
              "3          हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते  \n",
              "4             इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।  \n",
              "...                                                   ...  \n",
              "127602  आर्ट डेको शैली के निर्माण मैरीन ड्राइव और ओवल ...  \n",
              "127603                    और अपने गालों में डाल लेते हैं।  \n",
              "127604  जहां तक गंधक के अन्य उत्पादों का प्रश्न है , द...  \n",
              "127605  Zरचना-प्रकिया को उसने एक पहेली में यों बांधा है .  \n",
              "127606  हाल ही में उन्हें सरकारी ठेका मिला है करीब सौ ...  \n",
              "\n",
              "[127607 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv(\"/content/Hindi_English_Truncated_Corpus.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCQROM2NBIIr"
      },
      "outputs": [],
      "source": [
        "df.drop(['source'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "2Z2shKanPcpO",
        "outputId": "4ea75653-481c-4006-95e2-2cf52ab17c35"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-318b04da-04d9-40ff-8cd0-4032d95fa1b7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>english_sentence</th>\n",
              "      <th>hindi_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>politicians do not have permission to do what ...</td>\n",
              "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I'd like to tell you about one such child,</td>\n",
              "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This percentage is even greater than the perce...</td>\n",
              "      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what we really mean is that they're bad at not...</td>\n",
              "      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>.The ending portion of these Vedas is called U...</td>\n",
              "      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127602</th>\n",
              "      <td>Examples of art deco construction can be found...</td>\n",
              "      <td>आर्ट डेको शैली के निर्माण मैरीन ड्राइव और ओवल ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127603</th>\n",
              "      <td>and put it in our cheeks.</td>\n",
              "      <td>और अपने गालों में डाल लेते हैं।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127604</th>\n",
              "      <td>As for the other derivatives of sulphur , the ...</td>\n",
              "      <td>जहां तक गंधक के अन्य उत्पादों का प्रश्न है , द...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127605</th>\n",
              "      <td>its complicated functioning is defined thus in...</td>\n",
              "      <td>Zरचना-प्रकिया को उसने एक पहेली में यों बांधा है .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127606</th>\n",
              "      <td>They've just won four government contracts to ...</td>\n",
              "      <td>हाल ही में उन्हें सरकारी ठेका मिला है करीब सौ ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>127607 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-318b04da-04d9-40ff-8cd0-4032d95fa1b7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-318b04da-04d9-40ff-8cd0-4032d95fa1b7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-318b04da-04d9-40ff-8cd0-4032d95fa1b7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                         english_sentence  \\\n",
              "0       politicians do not have permission to do what ...   \n",
              "1              I'd like to tell you about one such child,   \n",
              "2       This percentage is even greater than the perce...   \n",
              "3       what we really mean is that they're bad at not...   \n",
              "4       .The ending portion of these Vedas is called U...   \n",
              "...                                                   ...   \n",
              "127602  Examples of art deco construction can be found...   \n",
              "127603                          and put it in our cheeks.   \n",
              "127604  As for the other derivatives of sulphur , the ...   \n",
              "127605  its complicated functioning is defined thus in...   \n",
              "127606  They've just won four government contracts to ...   \n",
              "\n",
              "                                           hindi_sentence  \n",
              "0       राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...  \n",
              "1       मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...  \n",
              "2        यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।  \n",
              "3          हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते  \n",
              "4             इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।  \n",
              "...                                                   ...  \n",
              "127602  आर्ट डेको शैली के निर्माण मैरीन ड्राइव और ओवल ...  \n",
              "127603                    और अपने गालों में डाल लेते हैं।  \n",
              "127604  जहां तक गंधक के अन्य उत्पादों का प्रश्न है , द...  \n",
              "127605  Zरचना-प्रकिया को उसने एक पहेली में यों बांधा है .  \n",
              "127606  हाल ही में उन्हें सरकारी ठेका मिला है करीब सौ ...  \n",
              "\n",
              "[127607 rows x 2 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ddUCyklR3Nu",
        "outputId": "a61b2268-e7c9-494e-b976-8de321a198a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "english_sentence    2\n",
              "hindi_sentence      0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.isnull(df).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHt27vmYR6b2"
      },
      "outputs": [],
      "source": [
        "df=df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrPJNkqJSNiE",
        "outputId": "0136fe87-4fe1-4257-d7a0-0178871a6e39"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "english_sentence    0\n",
              "hindi_sentence      0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.isnull(df).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLRQcJa9Sd9m",
        "outputId": "95cc0595-65b3-4bca-a982-5e5f39063fb1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Laughter)                                                                                                                                                    555\n",
              "(Applause)                                                                                                                                                    373\n",
              "Thank you.                                                                                                                                                     98\n",
              "History                                                                                                                                                        59\n",
              "(Music)                                                                                                                                                        44\n",
              "                                                                                                                                                             ... \n",
              "But when it came to personal mattters it was a different story .                                                                                                1\n",
              "Layered and High-quality video                                                                                                                                  1\n",
              "In the year 1955 Mahadevi established the 'Sahityakar sansad' in Allahabad and along with P. Ilachandra Joshi undertook the responsibility of 'Sahityakar'      1\n",
              "The halved chromo- somes grow to full size , resulting in two cells ,                                                                                           1\n",
              "They've just won four government contracts to build off their 100 ambulances,                                                                                   1\n",
              "Name: english_sentence, Length: 124317, dtype: int64"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['english_sentence'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSP0LUp0TuIf",
        "outputId": "d6e5e799-e350-4f7a-ac55-5c882132ddb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Duplicate Rows :\n",
            "                english_sentence         hindi_sentence\n",
            "595            Internet in India        भारत मे अंतरजाल\n",
            "652                   (Laughter)                (ठहाके)\n",
            "992                   (Laughter)                 (हंसी)\n",
            "1008                  (Laughter)                 (हँसी)\n",
            "1135                  (Laughter)                 (हंसी)\n",
            "...                          ...                    ...\n",
            "127495                   Devotee                   भक्त\n",
            "127516  Pashtun (Pathan) (15.4%)  पश्तून (पठान) (15.4%)\n",
            "127519            Deputy Speaker               उपस्पीकर\n",
            "127544                (Laughter)                 (हंसी)\n",
            "127598                Thank you.              धन्यवाद |\n",
            "\n",
            "[2780 rows x 2 columns]\n",
            "124825\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py:311: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  return func(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "#checking duplicate values\n",
        "data=df\n",
        "dupli=df[df.duplicated()]\n",
        "data.drop_duplicates(inplace = True)\n",
        "print(\"Duplicate Rows :\")\n",
        "\n",
        "print(dupli)\n",
        "print(len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u09r9W8pYTwr",
        "outputId": "7d30d27a-db49-4052-c134-99ae06129ec9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Duplicate Rows :\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py:311: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  return func(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "dupli=df[df.duplicated()]\n",
        "data.drop_duplicates(inplace = True)\n",
        "print(\"Duplicate Rows :\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfZWAKLtV-_4"
      },
      "outputs": [],
      "source": [
        "# checking duplicated data\n",
        "isDuplicated = df.duplicated().any()\n",
        "if isDuplicated:\n",
        "    total_duplicates = df.duplicated().sum()\n",
        "    print(\"Total duplicate rows are: \",total_duplicates)\n",
        "    df.drop_duplicates(inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FH-YA4Gza1Lb",
        "outputId": "21d45bbb-3930-4d39-e195-098e25a56ec4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbvBdpuEYYJA",
        "outputId": "2e2cddd4-6ebd-489d-b449-b5b4c97e3fc4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Eighty', 'seven', 'miles', 'to', 'go', 'yet', 'Onward']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "tokenizer.tokenize('Eighty-seven miles to go, yet.  Onward!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka-6OUEEeX3D",
        "outputId": "2a985415-e857-4e76-9f7b-709c4b33ca1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "print(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OPfmv31Ia8Bp"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "english_sentence=[]\n",
        "for i in df[\"english_sentence\"]:\n",
        "  s=i.lower()\n",
        "  print(tokenizer.tokenize(s))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')"
      ],
      "metadata": {
        "id": "3sCVuMguHAPy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    w = w.rstrip().strip()\n",
        "    return w"
      ],
      "metadata": {
        "id": "sc_UH0MKHCFC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hindi_preprocess_sentence(w):\n",
        "    w = w.rstrip().strip()\n",
        "    return w"
      ],
      "metadata": {
        "id": "YTxgUMaeHEqy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH='/content/Hindi_English_Truncated_Corpus.csv'\n",
        "def create_dataset(path=PATH):\n",
        "    lines=pd.read_csv(path,encoding='utf-8')\n",
        "    lines=lines.dropna()\n",
        "    lines = lines[lines['source']=='ted']\n",
        "    en = []\n",
        "    hd = []\n",
        "    for i, j in zip(lines['english_sentence'], lines['hindi_sentence']):\n",
        "        en_1 = [preprocess_sentence(w) for w in i.split(' ')]\n",
        "        en_1.append('<end>')\n",
        "        en_1.insert(0, '<start>')\n",
        "        hd_1 = [hindi_preprocess_sentence(w) for w in j.split(' ')]\n",
        "        hd_1.append('<end>')\n",
        "        hd_1.insert(0, '<start>')\n",
        "        en.append(en_1)\n",
        "        hd.append(hd_1)\n",
        "    return hd, en"
      ],
      "metadata": {
        "id": "KDZK2dW-JWNy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "metadata": {
        "id": "y0_VgMg5Jfby"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
        "  return tensor, lang_tokenizer"
      ],
      "metadata": {
        "id": "oURm7jKlJflf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(path=PATH):\n",
        "    targ_lang, inp_lang = create_dataset(path)\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "metadata": {
        "id": "vGSFc3YtJ4XK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(PATH)\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
      ],
      "metadata": {
        "id": "_2zHz9wsJ5FC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "input_tensor_train,input_tensor_val,target_tensor_train,target_tensor_val=train_test_split(input_tensor,target_tensor,test_size=0.2)\n",
        "print(len(input_tensor_train), len(input_tensor_val),len(target_tensor_train),len(target_tensor_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WbWo7ivKW1Q",
        "outputId": "9ddd597f-0af4-4209-9aa0-71bb222e2fcc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31904 7977 31904 7977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "    \n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GGl6G5RMzMI",
        "outputId": "8e7a71e6-37ea-4f5e-fe47-edfa19b7e131"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "3 ----> the\n",
            "14420 ----> throughput\n",
            "801 ----> rate\n",
            "139 ----> right\n",
            "73 ----> now\n",
            "11 ----> is\n",
            "187 ----> .\n",
            "221 ----> million\n",
            "3540 ----> nets ,\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "3 ----> और\n",
            "1557 ----> १५\n",
            "422 ----> लाख\n",
            "4286 ----> मच्छरदानियाँ\n",
            "3819 ----> प्रतिवर्ष\n",
            "9 ----> की\n",
            "648 ----> दर\n",
            "23 ----> पर\n",
            "1027 ----> उत्पादन\n",
            "37 ----> हो\n",
            "55 ----> रहा\n",
            "19 ----> है,\n",
            "2 ----> <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Dataset"
      ],
      "metadata": {
        "id": "52zUWcG3grKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 128\n",
        "units = 256\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "OxVbfIHNfSnr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder"
      ],
      "metadata": {
        "id": "JWiGayfDg0f-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "tp1vQvfYgpv9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention Mechanism"
      ],
      "metadata": {
        "id": "T3m28KNQF_8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "    return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "LKHaQjCDF-Nr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder"
      ],
      "metadata": {
        "id": "d1oFpLi0GEtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "    x = self.embedding(x)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "    output, state = self.gru(x)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    x = self.fc(output)\n",
        "    return x, state, attention_weights\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "8URYZ4u4GDFu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer"
      ],
      "metadata": {
        "id": "fllBn6TkGNCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "#   print(type(mask))\n",
        "  loss_ *= mask\n",
        "  return tf.reduce_mean(loss_)\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "metadata": {
        "id": "7cU6VvmKGJmQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "QjGlmTF9GT4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "    # Teacher forcing\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))      \n",
        "  return batch_loss"
      ],
      "metadata": {
        "id": "YPG0R-t6GQSH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "EPOCHS = 100\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                     batch,\n",
        "                                                     batch_loss.numpy()))\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQyiZ5saG5tB",
        "outputId": "cd2a21a1-53ff-46c6-dc2d-ba1cdd9ecc03"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 0.4092\n",
            "Epoch 1 Batch 100 Loss 0.4208\n",
            "Epoch 1 Batch 200 Loss 0.5287\n",
            "Epoch 1 Batch 300 Loss 0.4703\n",
            "Epoch 1 Batch 400 Loss 0.5064\n",
            "Epoch 1 Loss 0.5065\n",
            "Time taken for 1 epoch 44.953386545181274 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.4599\n",
            "Epoch 2 Batch 100 Loss 0.4080\n",
            "Epoch 2 Batch 200 Loss 0.4723\n",
            "Epoch 2 Batch 300 Loss 0.4616\n",
            "Epoch 2 Batch 400 Loss 0.4404\n",
            "Epoch 2 Loss 0.4745\n",
            "Time taken for 1 epoch 45.136613607406616 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.3819\n",
            "Epoch 3 Batch 100 Loss 0.4358\n",
            "Epoch 3 Batch 200 Loss 0.4308\n",
            "Epoch 3 Batch 300 Loss 0.5207\n",
            "Epoch 3 Batch 400 Loss 0.4162\n",
            "Epoch 3 Loss 0.4450\n",
            "Time taken for 1 epoch 45.05312395095825 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.4464\n",
            "Epoch 4 Batch 100 Loss 0.4019\n",
            "Epoch 4 Batch 200 Loss 0.3861\n",
            "Epoch 4 Batch 300 Loss 0.4365\n",
            "Epoch 4 Batch 400 Loss 0.4132\n",
            "Epoch 4 Loss 0.4133\n",
            "Time taken for 1 epoch 45.5749089717865 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.3768\n",
            "Epoch 5 Batch 100 Loss 0.3072\n",
            "Epoch 5 Batch 200 Loss 0.4034\n",
            "Epoch 5 Batch 300 Loss 0.3334\n",
            "Epoch 5 Batch 400 Loss 0.3696\n",
            "Epoch 5 Loss 0.3865\n",
            "Time taken for 1 epoch 45.483611822128296 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.3207\n",
            "Epoch 6 Batch 100 Loss 0.3729\n",
            "Epoch 6 Batch 200 Loss 0.3679\n",
            "Epoch 6 Batch 300 Loss 0.4054\n",
            "Epoch 6 Batch 400 Loss 0.3548\n",
            "Epoch 6 Loss 0.3642\n",
            "Time taken for 1 epoch 46.005677223205566 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.3075\n",
            "Epoch 7 Batch 100 Loss 0.3472\n",
            "Epoch 7 Batch 200 Loss 0.2847\n",
            "Epoch 7 Batch 300 Loss 0.3446\n",
            "Epoch 7 Batch 400 Loss 0.3705\n",
            "Epoch 7 Loss 0.3392\n",
            "Time taken for 1 epoch 45.64567708969116 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.2976\n",
            "Epoch 8 Batch 100 Loss 0.3298\n",
            "Epoch 8 Batch 200 Loss 0.3006\n",
            "Epoch 8 Batch 300 Loss 0.2976\n",
            "Epoch 8 Batch 400 Loss 0.3268\n",
            "Epoch 8 Loss 0.3173\n",
            "Time taken for 1 epoch 46.074209213256836 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.2428\n",
            "Epoch 9 Batch 100 Loss 0.2841\n",
            "Epoch 9 Batch 200 Loss 0.3272\n",
            "Epoch 9 Batch 300 Loss 0.2968\n",
            "Epoch 9 Batch 400 Loss 0.3027\n",
            "Epoch 9 Loss 0.2965\n",
            "Time taken for 1 epoch 45.62279438972473 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.2466\n",
            "Epoch 10 Batch 100 Loss 0.2721\n",
            "Epoch 10 Batch 200 Loss 0.2735\n",
            "Epoch 10 Batch 300 Loss 0.2720\n",
            "Epoch 10 Batch 400 Loss 0.3221\n",
            "Epoch 10 Loss 0.2782\n",
            "Time taken for 1 epoch 46.035184144973755 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.2240\n",
            "Epoch 11 Batch 100 Loss 0.1936\n",
            "Epoch 11 Batch 200 Loss 0.2937\n",
            "Epoch 11 Batch 300 Loss 0.2502\n",
            "Epoch 11 Batch 400 Loss 0.2743\n",
            "Epoch 11 Loss 0.2625\n",
            "Time taken for 1 epoch 45.63765549659729 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.2067\n",
            "Epoch 12 Batch 100 Loss 0.2539\n",
            "Epoch 12 Batch 200 Loss 0.2456\n",
            "Epoch 12 Batch 300 Loss 0.2452\n",
            "Epoch 12 Batch 400 Loss 0.2837\n",
            "Epoch 12 Loss 0.2504\n",
            "Time taken for 1 epoch 46.047895431518555 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.2227\n",
            "Epoch 13 Batch 100 Loss 0.2293\n",
            "Epoch 13 Batch 200 Loss 0.2478\n",
            "Epoch 13 Batch 300 Loss 0.2418\n",
            "Epoch 13 Batch 400 Loss 0.2199\n",
            "Epoch 13 Loss 0.2314\n",
            "Time taken for 1 epoch 45.60423803329468 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.2102\n",
            "Epoch 14 Batch 100 Loss 0.1743\n",
            "Epoch 14 Batch 200 Loss 0.2137\n",
            "Epoch 14 Batch 300 Loss 0.2529\n",
            "Epoch 14 Batch 400 Loss 0.2142\n",
            "Epoch 14 Loss 0.2164\n",
            "Time taken for 1 epoch 45.96459412574768 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.1512\n",
            "Epoch 15 Batch 100 Loss 0.2478\n",
            "Epoch 15 Batch 200 Loss 0.1667\n",
            "Epoch 15 Batch 300 Loss 0.2406\n",
            "Epoch 15 Batch 400 Loss 0.2170\n",
            "Epoch 15 Loss 0.2018\n",
            "Time taken for 1 epoch 45.65497398376465 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.1574\n",
            "Epoch 16 Batch 100 Loss 0.1648\n",
            "Epoch 16 Batch 200 Loss 0.1571\n",
            "Epoch 16 Batch 300 Loss 0.2063\n",
            "Epoch 16 Batch 400 Loss 0.1293\n",
            "Epoch 16 Loss 0.1899\n",
            "Time taken for 1 epoch 45.97642660140991 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.1573\n",
            "Epoch 17 Batch 100 Loss 0.2076\n",
            "Epoch 17 Batch 200 Loss 0.1961\n",
            "Epoch 17 Batch 300 Loss 0.1995\n",
            "Epoch 17 Batch 400 Loss 0.1818\n",
            "Epoch 17 Loss 0.1766\n",
            "Time taken for 1 epoch 45.61442017555237 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.1512\n",
            "Epoch 18 Batch 100 Loss 0.1833\n",
            "Epoch 18 Batch 200 Loss 0.1391\n",
            "Epoch 18 Batch 300 Loss 0.1840\n",
            "Epoch 18 Batch 400 Loss 0.1831\n",
            "Epoch 18 Loss 0.1641\n",
            "Time taken for 1 epoch 46.006855726242065 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.1357\n",
            "Epoch 19 Batch 100 Loss 0.1317\n",
            "Epoch 19 Batch 200 Loss 0.1700\n",
            "Epoch 19 Batch 300 Loss 0.1427\n",
            "Epoch 19 Batch 400 Loss 0.1233\n",
            "Epoch 19 Loss 0.1562\n",
            "Time taken for 1 epoch 45.70936298370361 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.1686\n",
            "Epoch 20 Batch 100 Loss 0.1705\n",
            "Epoch 20 Batch 200 Loss 0.1903\n",
            "Epoch 20 Batch 300 Loss 0.1686\n",
            "Epoch 20 Batch 400 Loss 0.1705\n",
            "Epoch 20 Loss 0.1465\n",
            "Time taken for 1 epoch 46.015228271484375 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.1231\n",
            "Epoch 21 Batch 100 Loss 0.1471\n",
            "Epoch 21 Batch 200 Loss 0.1470\n",
            "Epoch 21 Batch 300 Loss 0.1436\n",
            "Epoch 21 Batch 400 Loss 0.1664\n",
            "Epoch 21 Loss 0.1411\n",
            "Time taken for 1 epoch 45.642308712005615 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.1067\n",
            "Epoch 22 Batch 100 Loss 0.1627\n",
            "Epoch 22 Batch 200 Loss 0.1595\n",
            "Epoch 22 Batch 300 Loss 0.1554\n",
            "Epoch 22 Batch 400 Loss 0.1049\n",
            "Epoch 22 Loss 0.1442\n",
            "Time taken for 1 epoch 46.08309984207153 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.1521\n",
            "Epoch 23 Batch 100 Loss 0.0933\n",
            "Epoch 23 Batch 200 Loss 0.1078\n",
            "Epoch 23 Batch 300 Loss 0.1152\n",
            "Epoch 23 Batch 400 Loss 0.1096\n",
            "Epoch 23 Loss 0.1261\n",
            "Time taken for 1 epoch 45.69775414466858 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.0822\n",
            "Epoch 24 Batch 100 Loss 0.1049\n",
            "Epoch 24 Batch 200 Loss 0.0933\n",
            "Epoch 24 Batch 300 Loss 0.0978\n",
            "Epoch 24 Batch 400 Loss 0.1405\n",
            "Epoch 24 Loss 0.1131\n",
            "Time taken for 1 epoch 46.036152839660645 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.1464\n",
            "Epoch 25 Batch 100 Loss 0.1080\n",
            "Epoch 25 Batch 200 Loss 0.1310\n",
            "Epoch 25 Batch 300 Loss 0.1151\n",
            "Epoch 25 Batch 400 Loss 0.0888\n",
            "Epoch 25 Loss 0.1055\n",
            "Time taken for 1 epoch 45.70067000389099 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.0964\n",
            "Epoch 26 Batch 100 Loss 0.1162\n",
            "Epoch 26 Batch 200 Loss 0.0880\n",
            "Epoch 26 Batch 300 Loss 0.0900\n",
            "Epoch 26 Batch 400 Loss 0.0929\n",
            "Epoch 26 Loss 0.0986\n",
            "Time taken for 1 epoch 46.097108364105225 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.1111\n",
            "Epoch 27 Batch 100 Loss 0.0910\n",
            "Epoch 27 Batch 200 Loss 0.1083\n",
            "Epoch 27 Batch 300 Loss 0.0887\n",
            "Epoch 27 Batch 400 Loss 0.0909\n",
            "Epoch 27 Loss 0.0954\n",
            "Time taken for 1 epoch 45.687655210494995 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.0790\n",
            "Epoch 28 Batch 100 Loss 0.1009\n",
            "Epoch 28 Batch 200 Loss 0.0845\n",
            "Epoch 28 Batch 300 Loss 0.0724\n",
            "Epoch 28 Batch 400 Loss 0.1082\n",
            "Epoch 28 Loss 0.0926\n",
            "Time taken for 1 epoch 46.02118945121765 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.0801\n",
            "Epoch 29 Batch 100 Loss 0.0902\n",
            "Epoch 29 Batch 200 Loss 0.1091\n",
            "Epoch 29 Batch 300 Loss 0.0898\n",
            "Epoch 29 Batch 400 Loss 0.0945\n",
            "Epoch 29 Loss 0.0893\n",
            "Time taken for 1 epoch 45.6422438621521 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.0952\n",
            "Epoch 30 Batch 100 Loss 0.0993\n",
            "Epoch 30 Batch 200 Loss 0.1208\n",
            "Epoch 30 Batch 300 Loss 0.0863\n",
            "Epoch 30 Batch 400 Loss 0.0928\n",
            "Epoch 30 Loss 0.0853\n",
            "Time taken for 1 epoch 46.013922452926636 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.0663\n",
            "Epoch 31 Batch 100 Loss 0.0651\n",
            "Epoch 31 Batch 200 Loss 0.0917\n",
            "Epoch 31 Batch 300 Loss 0.0939\n",
            "Epoch 31 Batch 400 Loss 0.0765\n",
            "Epoch 31 Loss 0.0791\n",
            "Time taken for 1 epoch 45.608126401901245 sec\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.0607\n",
            "Epoch 32 Batch 100 Loss 0.0740\n",
            "Epoch 32 Batch 200 Loss 0.0665\n",
            "Epoch 32 Batch 300 Loss 0.0673\n",
            "Epoch 32 Batch 400 Loss 0.0692\n",
            "Epoch 32 Loss 0.0724\n",
            "Time taken for 1 epoch 46.04673790931702 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.0660\n",
            "Epoch 33 Batch 100 Loss 0.0626\n",
            "Epoch 33 Batch 200 Loss 0.0800\n",
            "Epoch 33 Batch 300 Loss 0.0757\n",
            "Epoch 33 Batch 400 Loss 0.0661\n",
            "Epoch 33 Loss 0.0697\n",
            "Time taken for 1 epoch 45.677229166030884 sec\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.0549\n",
            "Epoch 34 Batch 100 Loss 0.0519\n",
            "Epoch 34 Batch 200 Loss 0.0514\n",
            "Epoch 34 Batch 300 Loss 0.0606\n",
            "Epoch 34 Batch 400 Loss 0.0729\n",
            "Epoch 34 Loss 0.0667\n",
            "Time taken for 1 epoch 46.07283401489258 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.0531\n",
            "Epoch 35 Batch 100 Loss 0.0496\n",
            "Epoch 35 Batch 200 Loss 0.0740\n",
            "Epoch 35 Batch 300 Loss 0.0678\n",
            "Epoch 35 Batch 400 Loss 0.0542\n",
            "Epoch 35 Loss 0.0643\n",
            "Time taken for 1 epoch 45.65873742103577 sec\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.0464\n",
            "Epoch 36 Batch 100 Loss 0.0651\n",
            "Epoch 36 Batch 200 Loss 0.0524\n",
            "Epoch 36 Batch 300 Loss 0.0574\n",
            "Epoch 36 Batch 400 Loss 0.0490\n",
            "Epoch 36 Loss 0.0615\n",
            "Time taken for 1 epoch 46.076664209365845 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.0461\n",
            "Epoch 37 Batch 100 Loss 0.0640\n",
            "Epoch 37 Batch 200 Loss 0.0554\n",
            "Epoch 37 Batch 300 Loss 0.0522\n",
            "Epoch 37 Batch 400 Loss 0.0542\n",
            "Epoch 37 Loss 0.0649\n",
            "Time taken for 1 epoch 45.58663725852966 sec\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.0608\n",
            "Epoch 38 Batch 100 Loss 0.0571\n",
            "Epoch 38 Batch 200 Loss 0.0518\n",
            "Epoch 38 Batch 300 Loss 0.0564\n",
            "Epoch 38 Batch 400 Loss 0.0551\n",
            "Epoch 38 Loss 0.0611\n",
            "Time taken for 1 epoch 45.99113440513611 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.0779\n",
            "Epoch 39 Batch 100 Loss 0.0489\n",
            "Epoch 39 Batch 200 Loss 0.0417\n",
            "Epoch 39 Batch 300 Loss 0.0641\n",
            "Epoch 39 Batch 400 Loss 0.0633\n",
            "Epoch 39 Loss 0.0561\n",
            "Time taken for 1 epoch 45.63788414001465 sec\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.0498\n",
            "Epoch 40 Batch 100 Loss 0.0674\n",
            "Epoch 40 Batch 200 Loss 0.0479\n",
            "Epoch 40 Batch 300 Loss 0.0486\n",
            "Epoch 40 Batch 400 Loss 0.0571\n",
            "Epoch 40 Loss 0.0478\n",
            "Time taken for 1 epoch 45.98154807090759 sec\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.0492\n",
            "Epoch 41 Batch 100 Loss 0.0351\n",
            "Epoch 41 Batch 200 Loss 0.0376\n",
            "Epoch 41 Batch 300 Loss 0.0390\n",
            "Epoch 41 Batch 400 Loss 0.0436\n",
            "Epoch 41 Loss 0.0433\n",
            "Time taken for 1 epoch 45.66501045227051 sec\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.0309\n",
            "Epoch 42 Batch 100 Loss 0.0453\n",
            "Epoch 42 Batch 200 Loss 0.0365\n",
            "Epoch 42 Batch 300 Loss 0.0428\n",
            "Epoch 42 Batch 400 Loss 0.0574\n",
            "Epoch 42 Loss 0.0432\n",
            "Time taken for 1 epoch 46.01786184310913 sec\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.0321\n",
            "Epoch 43 Batch 100 Loss 0.0395\n",
            "Epoch 43 Batch 200 Loss 0.0463\n",
            "Epoch 43 Batch 300 Loss 0.0448\n",
            "Epoch 43 Batch 400 Loss 0.0592\n",
            "Epoch 43 Loss 0.0439\n",
            "Time taken for 1 epoch 45.632078886032104 sec\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.0275\n",
            "Epoch 44 Batch 100 Loss 0.0567\n",
            "Epoch 44 Batch 200 Loss 0.0375\n",
            "Epoch 44 Batch 300 Loss 0.0407\n",
            "Epoch 44 Batch 400 Loss 0.0549\n",
            "Epoch 44 Loss 0.0473\n",
            "Time taken for 1 epoch 46.0130341053009 sec\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.0303\n",
            "Epoch 45 Batch 100 Loss 0.0627\n",
            "Epoch 45 Batch 200 Loss 0.0283\n",
            "Epoch 45 Batch 300 Loss 0.0398\n",
            "Epoch 45 Batch 400 Loss 0.0474\n",
            "Epoch 45 Loss 0.0441\n",
            "Time taken for 1 epoch 45.595502614974976 sec\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.0286\n",
            "Epoch 46 Batch 100 Loss 0.0404\n",
            "Epoch 46 Batch 200 Loss 0.0279\n",
            "Epoch 46 Batch 300 Loss 0.0411\n",
            "Epoch 46 Batch 400 Loss 0.0305\n",
            "Epoch 46 Loss 0.0380\n",
            "Time taken for 1 epoch 46.01982021331787 sec\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.0345\n",
            "Epoch 47 Batch 100 Loss 0.0320\n",
            "Epoch 47 Batch 200 Loss 0.0325\n",
            "Epoch 47 Batch 300 Loss 0.0314\n",
            "Epoch 47 Batch 400 Loss 0.0406\n",
            "Epoch 47 Loss 0.0362\n",
            "Time taken for 1 epoch 45.61700963973999 sec\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.0321\n",
            "Epoch 48 Batch 100 Loss 0.0269\n",
            "Epoch 48 Batch 200 Loss 0.0266\n",
            "Epoch 48 Batch 300 Loss 0.0337\n",
            "Epoch 48 Batch 400 Loss 0.0339\n",
            "Epoch 48 Loss 0.0347\n",
            "Time taken for 1 epoch 46.03468680381775 sec\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.0251\n",
            "Epoch 49 Batch 100 Loss 0.0188\n",
            "Epoch 49 Batch 200 Loss 0.0347\n",
            "Epoch 49 Batch 300 Loss 0.0180\n",
            "Epoch 49 Batch 400 Loss 0.0322\n",
            "Epoch 49 Loss 0.0346\n",
            "Time taken for 1 epoch 45.66389513015747 sec\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.0233\n",
            "Epoch 50 Batch 100 Loss 0.0270\n",
            "Epoch 50 Batch 200 Loss 0.0386\n",
            "Epoch 50 Batch 300 Loss 0.0322\n",
            "Epoch 50 Batch 400 Loss 0.0313\n",
            "Epoch 50 Loss 0.0345\n",
            "Time taken for 1 epoch 46.10853934288025 sec\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.0412\n",
            "Epoch 51 Batch 100 Loss 0.0461\n",
            "Epoch 51 Batch 200 Loss 0.0344\n",
            "Epoch 51 Batch 300 Loss 0.0311\n",
            "Epoch 51 Batch 400 Loss 0.0318\n",
            "Epoch 51 Loss 0.0360\n",
            "Time taken for 1 epoch 45.59874749183655 sec\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.0330\n",
            "Epoch 52 Batch 100 Loss 0.0226\n",
            "Epoch 52 Batch 200 Loss 0.0373\n",
            "Epoch 52 Batch 300 Loss 0.0341\n",
            "Epoch 52 Batch 400 Loss 0.0336\n",
            "Epoch 52 Loss 0.0341\n",
            "Time taken for 1 epoch 46.00266194343567 sec\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.0222\n",
            "Epoch 53 Batch 100 Loss 0.0257\n",
            "Epoch 53 Batch 200 Loss 0.0305\n",
            "Epoch 53 Batch 300 Loss 0.0272\n",
            "Epoch 53 Batch 400 Loss 0.0271\n",
            "Epoch 53 Loss 0.0310\n",
            "Time taken for 1 epoch 45.61652088165283 sec\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.0178\n",
            "Epoch 54 Batch 100 Loss 0.0271\n",
            "Epoch 54 Batch 200 Loss 0.0297\n",
            "Epoch 54 Batch 300 Loss 0.0279\n",
            "Epoch 54 Batch 400 Loss 0.0414\n",
            "Epoch 54 Loss 0.0315\n",
            "Time taken for 1 epoch 46.06425070762634 sec\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.0152\n",
            "Epoch 55 Batch 100 Loss 0.0154\n",
            "Epoch 55 Batch 200 Loss 0.0415\n",
            "Epoch 55 Batch 300 Loss 0.0408\n",
            "Epoch 55 Batch 400 Loss 0.0246\n",
            "Epoch 55 Loss 0.0297\n",
            "Time taken for 1 epoch 45.6450560092926 sec\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.0331\n",
            "Epoch 56 Batch 100 Loss 0.0296\n",
            "Epoch 56 Batch 200 Loss 0.0314\n",
            "Epoch 56 Batch 300 Loss 0.0218\n",
            "Epoch 56 Batch 400 Loss 0.0287\n",
            "Epoch 56 Loss 0.0265\n",
            "Time taken for 1 epoch 46.05417323112488 sec\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.0303\n",
            "Epoch 57 Batch 100 Loss 0.0310\n",
            "Epoch 57 Batch 200 Loss 0.0299\n",
            "Epoch 57 Batch 300 Loss 0.0246\n",
            "Epoch 57 Batch 400 Loss 0.0306\n",
            "Epoch 57 Loss 0.0281\n",
            "Time taken for 1 epoch 45.61280369758606 sec\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.0288\n",
            "Epoch 58 Batch 100 Loss 0.0341\n",
            "Epoch 58 Batch 200 Loss 0.0357\n",
            "Epoch 58 Batch 300 Loss 0.0286\n",
            "Epoch 58 Batch 400 Loss 0.0364\n",
            "Epoch 58 Loss 0.0321\n",
            "Time taken for 1 epoch 45.95249080657959 sec\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.0271\n",
            "Epoch 59 Batch 100 Loss 0.0201\n",
            "Epoch 59 Batch 200 Loss 0.0327\n",
            "Epoch 59 Batch 300 Loss 0.0261\n",
            "Epoch 59 Batch 400 Loss 0.0244\n",
            "Epoch 59 Loss 0.0292\n",
            "Time taken for 1 epoch 45.66277599334717 sec\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.0205\n",
            "Epoch 60 Batch 100 Loss 0.0171\n",
            "Epoch 60 Batch 200 Loss 0.0258\n",
            "Epoch 60 Batch 300 Loss 0.0232\n",
            "Epoch 60 Batch 400 Loss 0.0231\n",
            "Epoch 60 Loss 0.0236\n",
            "Time taken for 1 epoch 46.02399277687073 sec\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.0103\n",
            "Epoch 61 Batch 100 Loss 0.0172\n",
            "Epoch 61 Batch 200 Loss 0.0192\n",
            "Epoch 61 Batch 300 Loss 0.0179\n",
            "Epoch 61 Batch 400 Loss 0.0426\n",
            "Epoch 61 Loss 0.0224\n",
            "Time taken for 1 epoch 45.63831901550293 sec\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.0274\n",
            "Epoch 62 Batch 100 Loss 0.0210\n",
            "Epoch 62 Batch 200 Loss 0.0211\n",
            "Epoch 62 Batch 300 Loss 0.0207\n",
            "Epoch 62 Batch 400 Loss 0.0184\n",
            "Epoch 62 Loss 0.0250\n",
            "Time taken for 1 epoch 45.973028898239136 sec\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.0213\n",
            "Epoch 63 Batch 100 Loss 0.0243\n",
            "Epoch 63 Batch 200 Loss 0.0271\n",
            "Epoch 63 Batch 300 Loss 0.0335\n",
            "Epoch 63 Batch 400 Loss 0.0249\n",
            "Epoch 63 Loss 0.0262\n",
            "Time taken for 1 epoch 45.59158635139465 sec\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.0273\n",
            "Epoch 64 Batch 100 Loss 0.0276\n",
            "Epoch 64 Batch 200 Loss 0.0272\n",
            "Epoch 64 Batch 300 Loss 0.0278\n",
            "Epoch 64 Batch 400 Loss 0.0188\n",
            "Epoch 64 Loss 0.0275\n",
            "Time taken for 1 epoch 45.972872495651245 sec\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.0163\n",
            "Epoch 65 Batch 100 Loss 0.0484\n",
            "Epoch 65 Batch 200 Loss 0.0222\n",
            "Epoch 65 Batch 300 Loss 0.0266\n",
            "Epoch 65 Batch 400 Loss 0.0266\n",
            "Epoch 65 Loss 0.0315\n",
            "Time taken for 1 epoch 45.66330051422119 sec\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.0378\n",
            "Epoch 66 Batch 100 Loss 0.0369\n",
            "Epoch 66 Batch 200 Loss 0.0346\n",
            "Epoch 66 Batch 300 Loss 0.0320\n",
            "Epoch 66 Batch 400 Loss 0.0344\n",
            "Epoch 66 Loss 0.0323\n",
            "Time taken for 1 epoch 46.01819634437561 sec\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.0138\n",
            "Epoch 67 Batch 100 Loss 0.0137\n",
            "Epoch 67 Batch 200 Loss 0.0233\n",
            "Epoch 67 Batch 300 Loss 0.0166\n",
            "Epoch 67 Batch 400 Loss 0.0217\n",
            "Epoch 67 Loss 0.0208\n",
            "Time taken for 1 epoch 45.568055629730225 sec\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.0091\n",
            "Epoch 68 Batch 100 Loss 0.0139\n",
            "Epoch 68 Batch 200 Loss 0.0173\n",
            "Epoch 68 Batch 300 Loss 0.0187\n",
            "Epoch 68 Batch 400 Loss 0.0128\n",
            "Epoch 68 Loss 0.0153\n",
            "Time taken for 1 epoch 46.01398181915283 sec\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.0135\n",
            "Epoch 69 Batch 100 Loss 0.0119\n",
            "Epoch 69 Batch 200 Loss 0.0095\n",
            "Epoch 69 Batch 300 Loss 0.0099\n",
            "Epoch 69 Batch 400 Loss 0.0146\n",
            "Epoch 69 Loss 0.0117\n",
            "Time taken for 1 epoch 45.709293603897095 sec\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.0098\n",
            "Epoch 70 Batch 100 Loss 0.0060\n",
            "Epoch 70 Batch 200 Loss 0.0070\n",
            "Epoch 70 Batch 300 Loss 0.0178\n",
            "Epoch 70 Batch 400 Loss 0.0068\n",
            "Epoch 70 Loss 0.0101\n",
            "Time taken for 1 epoch 46.05793380737305 sec\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.0069\n",
            "Epoch 71 Batch 100 Loss 0.0130\n",
            "Epoch 71 Batch 200 Loss 0.0118\n",
            "Epoch 71 Batch 300 Loss 0.0082\n",
            "Epoch 71 Batch 400 Loss 0.0131\n",
            "Epoch 71 Loss 0.0150\n",
            "Time taken for 1 epoch 45.67314386367798 sec\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.0281\n",
            "Epoch 72 Batch 100 Loss 0.0378\n",
            "Epoch 72 Batch 200 Loss 0.0539\n",
            "Epoch 72 Batch 300 Loss 0.0535\n",
            "Epoch 72 Batch 400 Loss 0.0549\n",
            "Epoch 72 Loss 0.0455\n",
            "Time taken for 1 epoch 46.05550789833069 sec\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.0375\n",
            "Epoch 73 Batch 100 Loss 0.0311\n",
            "Epoch 73 Batch 200 Loss 0.0430\n",
            "Epoch 73 Batch 300 Loss 0.0342\n",
            "Epoch 73 Batch 400 Loss 0.0335\n",
            "Epoch 73 Loss 0.0367\n",
            "Time taken for 1 epoch 45.648293018341064 sec\n",
            "\n",
            "Epoch 74 Batch 0 Loss 0.0273\n",
            "Epoch 74 Batch 100 Loss 0.0137\n",
            "Epoch 74 Batch 200 Loss 0.0366\n",
            "Epoch 74 Batch 300 Loss 0.0291\n",
            "Epoch 74 Batch 400 Loss 0.0270\n",
            "Epoch 74 Loss 0.0251\n",
            "Time taken for 1 epoch 46.036189556121826 sec\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.0199\n",
            "Epoch 75 Batch 100 Loss 0.0214\n",
            "Epoch 75 Batch 200 Loss 0.0143\n",
            "Epoch 75 Batch 300 Loss 0.0214\n",
            "Epoch 75 Batch 400 Loss 0.0111\n",
            "Epoch 75 Loss 0.0157\n",
            "Time taken for 1 epoch 45.66397404670715 sec\n",
            "\n",
            "Epoch 76 Batch 0 Loss 0.0171\n",
            "Epoch 76 Batch 100 Loss 0.0054\n",
            "Epoch 76 Batch 200 Loss 0.0137\n",
            "Epoch 76 Batch 300 Loss 0.0113\n",
            "Epoch 76 Batch 400 Loss 0.0115\n",
            "Epoch 76 Loss 0.0136\n",
            "Time taken for 1 epoch 46.02529764175415 sec\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.0266\n",
            "Epoch 77 Batch 100 Loss 0.0074\n",
            "Epoch 77 Batch 200 Loss 0.0136\n",
            "Epoch 77 Batch 300 Loss 0.0175\n",
            "Epoch 77 Batch 400 Loss 0.0221\n",
            "Epoch 77 Loss 0.0155\n",
            "Time taken for 1 epoch 45.608789682388306 sec\n",
            "\n",
            "Epoch 78 Batch 0 Loss 0.0181\n",
            "Epoch 78 Batch 100 Loss 0.0109\n",
            "Epoch 78 Batch 200 Loss 0.0135\n",
            "Epoch 78 Batch 300 Loss 0.0160\n",
            "Epoch 78 Batch 400 Loss 0.0142\n",
            "Epoch 78 Loss 0.0138\n",
            "Time taken for 1 epoch 45.97733235359192 sec\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.0096\n",
            "Epoch 79 Batch 100 Loss 0.0124\n",
            "Epoch 79 Batch 200 Loss 0.0121\n",
            "Epoch 79 Batch 300 Loss 0.0112\n",
            "Epoch 79 Batch 400 Loss 0.0134\n",
            "Epoch 79 Loss 0.0130\n",
            "Time taken for 1 epoch 45.58858823776245 sec\n",
            "\n",
            "Epoch 80 Batch 0 Loss 0.0127\n",
            "Epoch 80 Batch 100 Loss 0.0131\n",
            "Epoch 80 Batch 200 Loss 0.0146\n",
            "Epoch 80 Batch 300 Loss 0.0735\n",
            "Epoch 80 Batch 400 Loss 0.0538\n",
            "Epoch 80 Loss 0.0344\n",
            "Time taken for 1 epoch 45.9800488948822 sec\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.0676\n",
            "Epoch 81 Batch 100 Loss 0.0380\n",
            "Epoch 81 Batch 200 Loss 0.0420\n",
            "Epoch 81 Batch 300 Loss 0.0786\n",
            "Epoch 81 Batch 400 Loss 0.0312\n",
            "Epoch 81 Loss 0.0391\n",
            "Time taken for 1 epoch 45.61345052719116 sec\n",
            "\n",
            "Epoch 82 Batch 0 Loss 0.0293\n",
            "Epoch 82 Batch 100 Loss 0.0195\n",
            "Epoch 82 Batch 200 Loss 0.0192\n",
            "Epoch 82 Batch 300 Loss 0.0169\n",
            "Epoch 82 Batch 400 Loss 0.0225\n",
            "Epoch 82 Loss 0.0200\n",
            "Time taken for 1 epoch 45.989853620529175 sec\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.0140\n",
            "Epoch 83 Batch 100 Loss 0.0146\n",
            "Epoch 83 Batch 200 Loss 0.0170\n",
            "Epoch 83 Batch 300 Loss 0.0110\n",
            "Epoch 83 Batch 400 Loss 0.0097\n",
            "Epoch 83 Loss 0.0128\n",
            "Time taken for 1 epoch 45.63687801361084 sec\n",
            "\n",
            "Epoch 84 Batch 0 Loss 0.0146\n",
            "Epoch 84 Batch 100 Loss 0.0071\n",
            "Epoch 84 Batch 200 Loss 0.0085\n",
            "Epoch 84 Batch 300 Loss 0.0075\n",
            "Epoch 84 Batch 400 Loss 0.0049\n",
            "Epoch 84 Loss 0.0082\n",
            "Time taken for 1 epoch 46.00530242919922 sec\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.0040\n",
            "Epoch 85 Batch 100 Loss 0.0052\n",
            "Epoch 85 Batch 200 Loss 0.0072\n",
            "Epoch 85 Batch 300 Loss 0.0063\n",
            "Epoch 85 Batch 400 Loss 0.0092\n",
            "Epoch 85 Loss 0.0063\n",
            "Time taken for 1 epoch 45.61299753189087 sec\n",
            "\n",
            "Epoch 86 Batch 0 Loss 0.0048\n",
            "Epoch 86 Batch 100 Loss 0.0055\n",
            "Epoch 86 Batch 200 Loss 0.0069\n",
            "Epoch 86 Batch 300 Loss 0.0049\n",
            "Epoch 86 Batch 400 Loss 0.0035\n",
            "Epoch 86 Loss 0.0062\n",
            "Time taken for 1 epoch 46.041733741760254 sec\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.0055\n",
            "Epoch 87 Batch 100 Loss 0.0071\n",
            "Epoch 87 Batch 200 Loss 0.0048\n",
            "Epoch 87 Batch 300 Loss 0.0072\n",
            "Epoch 87 Batch 400 Loss 0.0059\n",
            "Epoch 87 Loss 0.0070\n",
            "Time taken for 1 epoch 45.64803671836853 sec\n",
            "\n",
            "Epoch 88 Batch 0 Loss 0.0091\n",
            "Epoch 88 Batch 100 Loss 0.0138\n",
            "Epoch 88 Batch 200 Loss 0.0329\n",
            "Epoch 88 Batch 300 Loss 0.0482\n",
            "Epoch 88 Batch 400 Loss 0.0616\n",
            "Epoch 88 Loss 0.0460\n",
            "Time taken for 1 epoch 46.03022909164429 sec\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.0573\n",
            "Epoch 89 Batch 100 Loss 0.0475\n",
            "Epoch 89 Batch 200 Loss 0.0252\n",
            "Epoch 89 Batch 300 Loss 0.0386\n",
            "Epoch 89 Batch 400 Loss 0.0368\n",
            "Epoch 89 Loss 0.0429\n",
            "Time taken for 1 epoch 45.62332463264465 sec\n",
            "\n",
            "Epoch 90 Batch 0 Loss 0.0173\n",
            "Epoch 90 Batch 100 Loss 0.0119\n",
            "Epoch 90 Batch 200 Loss 0.0213\n",
            "Epoch 90 Batch 300 Loss 0.0120\n",
            "Epoch 90 Batch 400 Loss 0.0214\n",
            "Epoch 90 Loss 0.0214\n",
            "Time taken for 1 epoch 46.16317105293274 sec\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.0149\n",
            "Epoch 91 Batch 100 Loss 0.0096\n",
            "Epoch 91 Batch 200 Loss 0.0062\n",
            "Epoch 91 Batch 300 Loss 0.0086\n",
            "Epoch 91 Batch 400 Loss 0.0110\n",
            "Epoch 91 Loss 0.0118\n",
            "Time taken for 1 epoch 45.63222289085388 sec\n",
            "\n",
            "Epoch 92 Batch 0 Loss 0.0075\n",
            "Epoch 92 Batch 100 Loss 0.0110\n",
            "Epoch 92 Batch 200 Loss 0.0131\n",
            "Epoch 92 Batch 300 Loss 0.0068\n",
            "Epoch 92 Batch 400 Loss 0.0075\n",
            "Epoch 92 Loss 0.0075\n",
            "Time taken for 1 epoch 46.10025405883789 sec\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.0048\n",
            "Epoch 93 Batch 100 Loss 0.0066\n",
            "Epoch 93 Batch 200 Loss 0.0064\n",
            "Epoch 93 Batch 300 Loss 0.0047\n",
            "Epoch 93 Batch 400 Loss 0.0045\n",
            "Epoch 93 Loss 0.0067\n",
            "Time taken for 1 epoch 45.67294359207153 sec\n",
            "\n",
            "Epoch 94 Batch 0 Loss 0.0037\n",
            "Epoch 94 Batch 100 Loss 0.0037\n",
            "Epoch 94 Batch 200 Loss 0.0075\n",
            "Epoch 94 Batch 300 Loss 0.0084\n",
            "Epoch 94 Batch 400 Loss 0.0096\n",
            "Epoch 94 Loss 0.0055\n",
            "Time taken for 1 epoch 46.026273250579834 sec\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.0049\n",
            "Epoch 95 Batch 100 Loss 0.0062\n",
            "Epoch 95 Batch 200 Loss 0.0089\n",
            "Epoch 95 Batch 300 Loss 0.0071\n",
            "Epoch 95 Batch 400 Loss 0.0067\n",
            "Epoch 95 Loss 0.0076\n",
            "Time taken for 1 epoch 45.60335063934326 sec\n",
            "\n",
            "Epoch 96 Batch 0 Loss 0.0107\n",
            "Epoch 96 Batch 100 Loss 0.0117\n",
            "Epoch 96 Batch 200 Loss 0.0375\n",
            "Epoch 96 Batch 300 Loss 0.0419\n",
            "Epoch 96 Batch 400 Loss 0.0606\n",
            "Epoch 96 Loss 0.0398\n",
            "Time taken for 1 epoch 45.999021768569946 sec\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.0395\n",
            "Epoch 97 Batch 100 Loss 0.0499\n",
            "Epoch 97 Batch 200 Loss 0.0295\n",
            "Epoch 97 Batch 300 Loss 0.0427\n",
            "Epoch 97 Batch 400 Loss 0.0539\n",
            "Epoch 97 Loss 0.0384\n",
            "Time taken for 1 epoch 45.63641691207886 sec\n",
            "\n",
            "Epoch 98 Batch 0 Loss 0.0355\n",
            "Epoch 98 Batch 100 Loss 0.0250\n",
            "Epoch 98 Batch 200 Loss 0.0251\n",
            "Epoch 98 Batch 300 Loss 0.0288\n",
            "Epoch 98 Batch 400 Loss 0.0192\n",
            "Epoch 98 Loss 0.0277\n",
            "Time taken for 1 epoch 46.033193588256836 sec\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.0127\n",
            "Epoch 99 Batch 100 Loss 0.0109\n",
            "Epoch 99 Batch 200 Loss 0.0141\n",
            "Epoch 99 Batch 300 Loss 0.0162\n",
            "Epoch 99 Batch 400 Loss 0.0097\n",
            "Epoch 99 Loss 0.0121\n",
            "Time taken for 1 epoch 45.66179704666138 sec\n",
            "\n",
            "Epoch 100 Batch 0 Loss 0.0043\n",
            "Epoch 100 Batch 100 Loss 0.0085\n",
            "Epoch 100 Batch 200 Loss 0.0064\n",
            "Epoch 100 Batch 300 Loss 0.0084\n",
            "Epoch 100 Batch 400 Loss 0.0052\n",
            "Epoch 100 Loss 0.0073\n",
            "Time taken for 1 epoch 46.041372537612915 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    result = ''\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result, sentence"
      ],
      "metadata": {
        "id": "23ghnRMkMQ7u"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "    result, sentence = evaluate(sentence)\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "    return result\n",
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD2WI_LKMUYO",
        "outputId": "d54aee54-9553-4db0-f54f-d442bdbc7fa1"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f9a4b375550>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "result=translate(u'politicians do not have permission to do what needs to be done.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqQUbcdCMVA6",
        "outputId": "1cf11e45-c55c-4a51-ff6b-6ad84ce063e0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: politicians do not have permission to do what needs to be done .\n",
            "Predicted translation: राजनीतिज्ञों के लिए जो हम सुनना चाहते हैं जितना करना चाहते हैं जितना नहीं करता है. <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = 'राजनेताओं को वह करने की अनुमति नहीं है जो करने की आवश्यकता है।'.split()\n",
        "result=str(result).split()\n",
        "print(reference)\n",
        "print(result)\n",
        "print((sentence_bleu(result, reference)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYZOlCpth09f",
        "outputId": "ca47d8f9-442e-45ec-8728-fd1be661b93f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['राजनेताओं', 'को', 'वह', 'करने', 'की', 'अनुमति', 'नहीं', 'है', 'जो', 'करने', 'की', 'आवश्यकता', 'है।']\n",
            "[\"['राजनीतिज्ञों',\", \"'के',\", \"'लिए',\", \"'जो',\", \"'हम',\", \"'सुनना',\", \"'चाहते',\", \"'हैं',\", \"'जितना',\", \"'करना',\", \"'चाहते',\", \"'हैं',\", \"'जितना',\", \"'नहीं',\", \"'करता',\", \"'है.',\", \"'<end>']\"]\n",
            "0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "English to Hindi Machine Translator Using GRU.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}